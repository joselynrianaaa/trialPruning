{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781fe76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting detectors\n",
      "  Downloading detectors-0.1.11-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from detectors) (10.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from detectors) (1.24.4)\n",
      "Collecting optuna (from detectors)\n",
      "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from detectors) (1.2.0)\n",
      "Collecting scikit-image (from detectors)\n",
      "  Downloading scikit_image-0.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting timm>=0.8.19.dev0 (from detectors)\n",
      "  Downloading timm-1.0.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m955.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from detectors) (2.3.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from detectors) (0.18.1)\n",
      "Collecting tqdm (from detectors)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from detectors) (1.12.0)\n",
      "Collecting accelerate (from detectors)\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from detectors) (5.9.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from detectors) (1.5.3)\n",
      "Collecting wilds (from detectors)\n",
      "  Downloading wilds-2.0.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting faiss-cpu (from detectors)\n",
      "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectors) (3.9.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm>=0.8.19.dev0->detectors) (6.0.1)\n",
      "Collecting huggingface_hub (from timm>=0.8.19.dev0->detectors)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting safetensors (from timm>=0.8.19.dev0->detectors)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->detectors) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->detectors) (12.5.40)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->detectors) (23.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectors) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectors) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectors) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectors) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectors) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectors) (2.9.0.post0)\n",
      "Collecting alembic>=1.5.0 (from optuna->detectors)\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna->detectors)\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sqlalchemy>=1.3.0 (from optuna->detectors)\n",
      "  Downloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->detectors) (2024.1)\n",
      "Collecting imageio>=2.33 (from scikit-image->detectors)\n",
      "  Downloading imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->detectors)\n",
      "  Downloading tifffile-2024.5.22-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->detectors)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->detectors) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->detectors) (3.3.0)\n",
      "Collecting ogb>=1.2.6 (from wilds->detectors)\n",
      "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting outdated>=0.2.0 (from wilds->detectors)\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna->detectors)\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.6->wilds->detectors) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.6->wilds->detectors) (1.26.18)\n",
      "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->wilds->detectors) (69.2.0)\n",
      "Collecting littleutils (from outdated>=0.2.0->wilds->detectors)\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->wilds->detectors) (2.31.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.3.0->optuna->detectors)\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.1->detectors) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->detectors) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->wilds->detectors) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->wilds->detectors) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->wilds->detectors) (2024.2.2)\n",
      "Downloading detectors-0.1.11-py3-none-any.whl (616 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.8/616.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_image-0.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wilds-2.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.2/126.2 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.34.1-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2024.5.22-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.5/225.5 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: littleutils\n",
      "  Building wheel for littleutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=e829e785498fb2ad1ca2f10c79788a2f73d528d49131c8e8879c06c89e04749b\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
      "Successfully built littleutils\n",
      "Installing collected packages: littleutils, tqdm, tifffile, safetensors, Mako, lazy-loader, imageio, greenlet, faiss-cpu, colorlog, sqlalchemy, scikit-image, outdated, huggingface_hub, alembic, optuna, ogb, accelerate, wilds, timm, detectors\n",
      "Successfully installed Mako-1.3.5 accelerate-0.31.0 alembic-1.13.1 colorlog-6.8.2 detectors-0.1.11 faiss-cpu-1.8.0 greenlet-3.0.3 huggingface_hub-0.23.3 imageio-2.34.1 lazy-loader-0.4 littleutils-0.2.2 ogb-1.3.6 optuna-3.6.1 outdated-0.2.2 safetensors-0.4.3 scikit-image-0.23.2 sqlalchemy-2.0.30 tifffile-2024.5.22 timm-1.0.3 tqdm-4.66.4 wilds-2.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5155b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: \"https://huggingface.co/edadaltocg/vgg16_bn_cifar10/resolve/main/pytorch_model.bin\" to /root/.cache/torch/hub/checkpoints/vgg16_bn_cifar10.pth\n",
      "100%|██████████| 56.2M/56.2M [00:04<00:00, 13.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import detectors\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "model = timm.create_model(\"vgg16_bn_cifar10\", pretrained=True)\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "model = timm.create_model(\"vgg16_bn_cifar10\", pretrained=False)\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3fc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Add this line\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from torchsummary import summary\n",
    "from thop import profile\n",
    "\n",
    "# Initialize random seed for reproducibility\n",
    "seed = 1787\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "th.manual_seed(seed)\n",
    "th.cuda.manual_seed(seed)\n",
    "th.cuda.manual_seed_all(seed)\n",
    "th.backends.cudnn.deterministic = True\n",
    "th.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set device\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Parameters\n",
    "epochs = 40\n",
    "custom_epochs = 15\n",
    "new_epochs = 30\n",
    "prune_percentage = [0.04] + [0.10]\n",
    "prune_limits = [1, 2]  # Desired minimum filter counts\n",
    "optim_lr = 0.0001\n",
    "lamda = 0.01\n",
    "\n",
    "# Data loaders\n",
    "trainloader = th.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('../data', download=True, train=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()])),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "testloader = th.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('../data', download=True, train=False,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()])),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "class PruningMethod:\n",
    "    def prune_filters(self, indices):\n",
    "        conv_layer = 0\n",
    "        for layer_name, layer_module in self.named_modules():\n",
    "            if isinstance(layer_module, th.nn.Conv2d):\n",
    "                if conv_layer == 0:\n",
    "                    in_channels = [i for i in range(layer_module.weight.shape[1])]\n",
    "                else:\n",
    "                    in_channels = indices[conv_layer - 1]\n",
    "\n",
    "                out_channels = indices[conv_layer]\n",
    "                layer_module.weight = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.weight.data.cpu().numpy()[out_channels])))\n",
    "\n",
    "                if layer_module.bias is not None:\n",
    "                    layer_module.bias = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.bias.data.cpu().numpy()[out_channels])).to('cuda'))\n",
    "\n",
    "                layer_module.weight = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.weight.data.numpy()[:, in_channels])).to('cuda'))\n",
    "\n",
    "                layer_module.in_channels = len(in_channels)\n",
    "                layer_module.out_channels = len(out_channels)\n",
    "                conv_layer += 1\n",
    "\n",
    "            if isinstance(layer_module, th.nn.BatchNorm2d):\n",
    "                out_channels = indices[conv_layer]\n",
    "                layer_module.weight = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.weight.data.cpu().numpy()[out_channels])).to('cuda'))\n",
    "                layer_module.bias = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.bias.data.cpu().numpy()[out_channels])).to('cuda'))\n",
    "                layer_module.running_mean = th.from_numpy(layer_module.running_mean.cpu().numpy()[out_channels]).to('cuda')\n",
    "                layer_module.running_var = th.from_numpy(layer_module.running_var.cpu().numpy()[out_channels]).to('cuda')\n",
    "                layer_module.num_features = len(out_channels)\n",
    "\n",
    "            if isinstance(layer_module, nn.Linear):\n",
    "                conv_layer -= 1\n",
    "                in_channels = indices[conv_layer]\n",
    "                weight_linear = layer_module.weight.data.cpu().numpy()\n",
    "                size = 4 * 4\n",
    "                expanded_in_channels = []\n",
    "                for i in in_channels:\n",
    "                    for j in range(size):\n",
    "                        expanded_in_channels.extend([i * size + j])\n",
    "                layer_module.weight = th.nn.Parameter(th.from_numpy(weight_linear[:, expanded_in_channels]).to('cuda'))\n",
    "                layer_module.in_features = len(expanded_in_channels)\n",
    "                break\n",
    "\n",
    "    def get_indices_topk(self, layer_bounds, i, prune_limit, prune_percentage):\n",
    "        indices = int(len(layer_bounds) * prune_percentage[i]) + 1\n",
    "        p = len(layer_bounds)\n",
    "        if (p - indices) < prune_limit:\n",
    "            remaining = p - prune_limit\n",
    "            indices = remaining\n",
    "        k = sorted(range(len(layer_bounds)), key=lambda j: layer_bounds[j])[:indices]\n",
    "        return k\n",
    "\n",
    "    def get_indices_bottomk(self, layer_bounds, i, prune_limit):\n",
    "        k = sorted(range(len(layer_bounds)), key=lambda j: layer_bounds[j])[-prune_limit:]\n",
    "        return k\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = th.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30], gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Load pre-trained model if available\n",
    "checkpoint = th.load('base.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "epoch_train_acc = checkpoint['train_acc']\n",
    "epoch_test_acc = checkpoint['test_acc']\n",
    "\n",
    "# Helper function to calculate cosine similarity between filters\n",
    "def calculate_cosine_similarity(layer_weights):\n",
    "    num_filters = layer_weights.shape[0]\n",
    "    flat_filters = layer_weights.reshape(num_filters, -1).cpu().numpy()\n",
    "    similarity_matrix = cosine_similarity(flat_filters)\n",
    "    return similarity_matrix\n",
    "\n",
    "# Get convolutional layers\n",
    "conv_layers = [module for module in model.modules() if isinstance(module, nn.Conv2d)]\n",
    "\n",
    "# Pruning loop\n",
    "continue_pruning = True\n",
    "prunes = 0\n",
    "best_train_acc = epoch_train_acc\n",
    "best_test_acc = epoch_test_acc\n",
    "\n",
    "while continue_pruning:\n",
    "    # Calculate cosine similarity for each layer\n",
    "    layer_similarities = []\n",
    "    for layer in conv_layers:\n",
    "        with th.no_grad():\n",
    "            similarity_matrix = calculate_cosine_similarity(layer.weight)\n",
    "            layer_similarities.append(similarity_matrix)\n",
    "\n",
    "    # Select filters to prune based on cosine similarity\n",
    "    inc_indices = []\n",
    "    unimp_indices = []\n",
    "    dec_indices = []\n",
    "    remaining_indices = []\n",
    "\n",
    "    for i, sim_matrix in enumerate(layer_similarities):\n",
    "        num_filters = sim_matrix.shape[0]\n",
    "        sim_flat = sim_matrix.flatten()\n",
    "        sorted_indices = np.argsort(sim_flat)[::-1]\n",
    "        selected_indices = []\n",
    "        for idx in sorted_indices:\n",
    "            if len(selected_indices) >= prune_limits[i]:\n",
    "                break\n",
    "            row, col = divmod(idx, num_filters)\n",
    "            if row != col and row not in selected_indices and col not in selected_indices:\n",
    "                selected_indices.extend([row, col])\n",
    "\n",
    "        inc_indices.append(selected_indices)\n",
    "        unimp_indices_layer = model.get_indices_topk(sim_matrix.sum(axis=0).tolist(), i, prune_limits[i], prune_percentage)\n",
    "        unimp_indices.append(unimp_indices_layer)\n",
    "        dec_indices.append(list(set(selected_indices + unimp_indices_layer)))\n",
    "        remaining_indices.append([j for j in range(num_filters) if j not in unimp_indices_layer])\n",
    "\n",
    "    # Custom regularization\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=optim_lr, momentum=0.9)\n",
    "    for epoch in range(custom_epochs):\n",
    "        train_acc = []\n",
    "        for inputs, targets in trainloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate regularization terms\n",
    "            reg = th.zeros(1).to(device)\n",
    "            for i, layer in enumerate(conv_layers):\n",
    "                dec_weight = sum(layer.weight[idx].norm(1) for idx in dec_indices[i])\n",
    "                inc_weight = sum(layer.weight[idx].norm(1) for idx in inc_indices[i])\n",
    "                reg += lamda * (dec_weight - inc_weight)\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets) + reg\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with th.no_grad():\n",
    "                y_hat = th.argmax(output, 1)\n",
    "                train_acc.append((y_hat == targets).sum().item())\n",
    "\n",
    "        epoch_train_acc = sum(train_acc) * 100 / len(trainloader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{custom_epochs}], Train Accuracy: {epoch_train_acc:.2f}%')\n",
    "\n",
    "        test_acc = []\n",
    "        with th.no_grad():\n",
    "            for inputs, targets in testloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                output = model(inputs)\n",
    "                y_hat = th.argmax(output, 1)\n",
    "                test_acc.append((y_hat == targets).sum().item())\n",
    "        epoch_test_acc = sum(test_acc) * 100 / len(testloader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{custom_epochs}], Test Accuracy: {epoch_test_acc:.2f}%')\n",
    "\n",
    "        if epoch_test_acc > best_test_acc:\n",
    "            best_train_acc = epoch_train_acc\n",
    "            best_test_acc = epoch_test_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "            best_opt_wts = optimizer.state_dict()\n",
    "            best_sch_wts = scheduler.state_dict()\n",
    "\n",
    "    # Prune filters\n",
    "    model.prune_filters(remaining_indices)\n",
    "\n",
    "    # Print remaining filters in each convolutional layer\n",
    "    for i, layer in enumerate(conv_layers):\n",
    "        print(f'Layer {i+1} - Remaining Filters: {layer.out_channels}')\n",
    "\n",
    "    # Check if desired filter counts are reached\n",
    "    continue_pruning = any(layer.out_channels > prune_limits[i] for i, layer in enumerate(conv_layers))\n",
    "\n",
    "    # Fine-tuning\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=optim_lr, momentum=0.9)\n",
    "    scheduler = th.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20], gamma=0.1)\n",
    "    for epoch in range(new_epochs):\n",
    "        train_acc = []\n",
    "        for inputs, targets in trainloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with th.no_grad():\n",
    "                y_hat = th.argmax(output, 1)\n",
    "                train_acc.append((y_hat == targets).sum().item())\n",
    "\n",
    "        epoch_train_acc = sum(train_acc) * 100 / len(trainloader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{new_epochs}], Train Accuracy: {epoch_train_acc:.2f}%')\n",
    "\n",
    "        test_acc = []\n",
    "        with th.no_grad():\n",
    "            for inputs, targets in testloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                output = model(inputs)\n",
    "                y_hat = th.argmax(output, 1)\n",
    "                test_acc.append((y_hat == targets).sum().item())\n",
    "        epoch_test_acc = sum(test_acc) * 100 / len(testloader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{new_epochs}], Test Accuracy: {epoch_test_acc:.2f}%')\n",
    "\n",
    "        if epoch_test_acc > best_test_acc:\n",
    "            best_train_acc = epoch_train_acc\n",
    "            best_test_acc = epoch_test_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "            best_opt_wts = optimizer.state_dict()\n",
    "            best_sch_wts = scheduler.state_dict()\n",
    "\n",
    "    prunes += 1\n",
    "\n",
    "# Save the pruned model\n",
    "th.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "    'train_acc': best_train_acc,\n",
    "    'test_acc': best_test_acc\n",
    "}, 'pruned_model.pth')\n",
    "\n",
    "print(\"Pruning completed successfully.\")\n",
    "\n",
    "# Print model summary\n",
    "summary(model, (1, 32, 32))\n",
    "\n",
    "# Calculate FLOPs and parameters\n",
    "dummy_input = th.randn(1, 1, 32, 32).to(device)\n",
    "flops, params = profile(model, inputs=(dummy_input,))\n",
    "print(f\"Total FLOPs: {flops}, Total Params: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268898c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 14 08:42:33 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 12.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:0F:00.0 Off |                   On |\n",
      "| N/A   32C    P0    77W / 400W |                  N/A |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n",
      "|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n",
      "|                  |                      |        ECC|                       |\n",
      "|==================+======================+===========+=======================|\n",
      "|  0    1   0   0  |    574MiB / 20096MiB | 42      0 |  3   0    2    0    0 |\n",
      "|                  |      4MiB / 32767MiB |           |                       |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809f3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
