{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fe00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LeNet-5 model for MNIST\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(50 * 4 * 4, 800)\n",
    "        self.fc2 = nn.Linear(800, 500)\n",
    "        self.fc3 = nn.Linear(500, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 50 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def get_lenet5():\n",
    "    return LeNet5()\n",
    "\n",
    "# Dataset preparation (MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Adjust the paths to point to the correct dataset directory\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79091d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for baseline model\n",
    "def train_baseline_model(net, trainloader, criterion, optimizer, epochs=40):\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        if epoch in [20, 30]:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] /= 10\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(trainloader):.4f}')\n",
    "\n",
    "# Testing function for baseline model\n",
    "def test_model(net, testloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Save model function\n",
    "def save_model(net, path):\n",
    "    torch.save(net.state_dict(), path)\n",
    "\n",
    "# Load model function\n",
    "def load_model(net, path):\n",
    "    net.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccf2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning method definition\n",
    "class PruningMethod(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = 'structured'\n",
    "\n",
    "    def __init__(self, filters_selected, dim=0):\n",
    "        self.filters_selected = filters_selected\n",
    "        self.dim = dim\n",
    "\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        if len(t.shape) == 4:\n",
    "            mask[self.filters_selected, :, :, :] = 0\n",
    "        elif len(t.shape) == 2:\n",
    "            mask[self.filters_selected, :] = 0\n",
    "        elif len(t.shape) == 1:\n",
    "            mask[self.filters_selected] = 0\n",
    "        return mask\n",
    "\n",
    "def prune_filters_by_similarity(net, prune_ratio_conv1, prune_ratio_conv2):\n",
    "    for name, module in net.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            filters = module.weight.data.view(module.out_channels, -1)\n",
    "            cos_sim = F.cosine_similarity(filters.unsqueeze(1), filters.unsqueeze(0), dim=-1).cpu().numpy()\n",
    "\n",
    "            np.fill_diagonal(cos_sim, 0)\n",
    "            cos_sim_triu = np.triu(cos_sim)\n",
    "            if 'conv1' in name:\n",
    "                num_filters_to_prune = int(prune_ratio_conv1 * module.out_channels)\n",
    "            elif 'conv2' in name:\n",
    "                num_filters_to_prune = int(prune_ratio_conv2 * module.out_channels)\n",
    "\n",
    "            high_sim_indices = np.argsort(-cos_sim_triu, axis=None)[:num_filters_to_prune]\n",
    "            high_sim_pairs = np.column_stack(np.unravel_index(high_sim_indices, cos_sim_triu.shape))\n",
    "\n",
    "            selected_filters = high_sim_pairs[:, 1]\n",
    "            PruningMethod.apply(module, 'weight', filters_selected=selected_filters)\n",
    "\n",
    "            print(f'Pruned {len(selected_filters)} filters from {module}')\n",
    "            print(f'Number of remaining filters: {module.out_channels - len(selected_filters)}')\n",
    "\n",
    "def calculate_similarity(t):\n",
    "    t = t.view(t.size(0), -1)\n",
    "    sim_matrix = torch.zeros(t.size(0), t.size(0)).cuda()\n",
    "    for i in range(t.size(0)):\n",
    "        for j in range(i + 1, t.size(0)):\n",
    "            sim_matrix[i, j] = F.cosine_similarity(t[i], t[j], dim=0)\n",
    "    return sim_matrix\n",
    "\n",
    "def calculate_flops_and_params(model, input_size):\n",
    "    input = torch.randn(1, 1, *input_size).cuda()\n",
    "    flops, params = profile(model, inputs=(input,))\n",
    "    return flops, params\n",
    "\n",
    "def iterative_pruning(net, trainloader, testloader, criterion, optimizer, prune_ratio_conv1, prune_ratio_conv2, prune_limit_conv1, prune_limit_conv2, alpha, beta, epochs=40):\n",
    "    best_accuracy = 0.0\n",
    "    best_model_path = 'best_model.pth'\n",
    "    initial_accuracy = test_model(net, testloader)\n",
    "\n",
    "    for prune_iter in range(int(max(prune_limit_conv1 / prune_ratio_conv1, prune_limit_conv2 / prune_ratio_conv2))):\n",
    "        print(f'Pruning Iteration {prune_iter + 1}')\n",
    "\n",
    "        # Load the best model before pruning\n",
    "        if prune_iter > 0:\n",
    "            load_model(net, best_model_path)\n",
    "        \n",
    "        prune_filters_by_similarity(net, prune_ratio_conv1, prune_ratio_conv2)\n",
    "\n",
    "        flops, params = calculate_flops_and_params(net, (28, 28))\n",
    "        print(f'FLOPs after pruning: {flops / 1e6:.2f}M')\n",
    "        print(f'Parameters after pruning: {params / 1e6:.2f}M')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if epoch in [10, 20]:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] /= 10\n",
    "            net.train()\n",
    "            running_loss = 0.0\n",
    "            old_running_loss = 0.0\n",
    "            for inputs, labels in trainloader:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                old_loss = criterion(outputs, labels)\n",
    "                old_running_loss += old_loss.item()\n",
    "\n",
    "                regularization_term = 0\n",
    "\n",
    "                for module in net.modules():\n",
    "                    if isinstance(module, nn.Conv2d):\n",
    "                        filters = module.weight.data.view(module.out_channels, -1)\n",
    "                        sim_matrix = calculate_similarity(filters)\n",
    "\n",
    "                        # Sort similarity values and select the top 2%\n",
    "                        sim_values = sim_matrix.view(-1)\n",
    "                        top_2_percent_idx = torch.topk(sim_values, int(0.02 * sim_values.numel()), largest=True).indices\n",
    "                        regularization_term += torch.exp(-torch.sum(sim_values[top_2_percent_idx]))\n",
    "\n",
    "                new_loss = old_loss + alpha * regularization_term\n",
    "                new_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += new_loss.item()\n",
    "            print(f'Pruning Iteration {prune_iter + 1}, Epoch [{epoch + 1}/{epochs}], Old Loss: {old_running_loss / len(trainloader):.4f}, New Loss: {running_loss / len(trainloader):.4f}')\n",
    "\n",
    "        accuracy = test_model(net, testloader)\n",
    "\n",
    "        # Save the best model based on accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            save_model(net, best_model_path)\n",
    "            print(f'Best model saved with accuracy: {best_accuracy:.2f}%')\n",
    "\n",
    "        # Stop if the accuracy drop is less than 2%\n",
    "        if abs(initial_accuracy - best_accuracy) < 2:\n",
    "            print(f'Pruning stopped as the accuracy drop is less than 2%. Final accuracy: {best_accuracy:.2f}%')\n",
    "            break\n",
    "\n",
    "    # Save the final pruned model\n",
    "    save_model(net, 'pruned_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "net = get_lenet5().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "# Train baseline model\n",
    "train_baseline_model(net, trainloader, criterion, optimizer, epochs=40)\n",
    "\n",
    "# Evaluate baseline model\n",
    "initial_accuracy = test_model(net, testloader)\n",
    "save_model(net, 'baseline_model.pth')\n",
    "save_model(net, 'best_model.pth')\n",
    "\n",
    "# Pruning configurations\n",
    "prune_ratio_conv1 = 0.04\n",
    "prune_ratio_conv2 = 0.12\n",
    "prune_limit_conv1 = 0.999\n",
    "prune_limit_conv2 = 0.999\n",
    "alpha = 0.01\n",
    "beta = 0.02\n",
    "\n",
    "# Perform iterative pruning with regularization\n",
    "iterative_pruning(net, trainloader, testloader, criterion, optimizer, prune_ratio_conv1, prune_ratio_conv2, prune_limit_conv1, prune_limit_conv2, alpha, beta, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002711d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
