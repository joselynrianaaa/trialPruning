{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ae473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.nn.functional as F  # Add this line\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from torchsummary import summary\n",
    "from thop import profile\n",
    "seed =1787\n",
    "random.seed(seed)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "th.manual_seed(seed)\n",
    "th.cuda.manual_seed(seed)\n",
    "th.cuda.manual_seed_all(seed)\n",
    "th.backends.cudnn.deterministic = True\n",
    "th.cuda.set_device(0)\n",
    "N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295dbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1787\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "th.manual_seed(seed)\n",
    "th.cuda.manual_seed(seed)\n",
    "th.cuda.manual_seed_all(seed)\n",
    "th.backends.cudnn.deterministic = True\n",
    "th.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755345c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8964ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 1\n",
    "custom_epochs = 1#15\n",
    "new_epochs = 1#30\n",
    "prune_percentage = [0.04] + [0.10]\n",
    "prune_limits = [1, 2]  # Desired minimum filter counts\n",
    "optim_lr = 0.0001\n",
    "lamda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac6e127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "th.cuda.set_device(0)\n",
    "gpu = th.cuda.is_available()\n",
    "if not gpu:\n",
    "    print('qqqq')\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = th.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform_test)\n",
    "    testloader = th.utils.data.DataLoader(testset, batch_size=100, shuffle=True, num_workers=2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264785c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    def weight_init(self, m):\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "            if self.a_type == 'relu':\n",
    "                init.kaiming_normal_(m.weight.data, nonlinearity=self.a_type)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            elif self.a_type == 'leaky_relu':\n",
    "                init.kaiming_normal_(m.weight.data, nonlinearity=self.a_type)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            elif self.a_type == 'tanh':\n",
    "                g = init.calculate_gain(self.a_type)\n",
    "                init.xavier_uniform_(m.weight.data, gain=g)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            elif self.a_type == 'sigmoid':\n",
    "                g = init.calculate_gain(self.a_type)\n",
    "                init.xavier_uniform_(m.weight.data, gain=g)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            else:\n",
    "                raise\n",
    "                return NotImplemented\n",
    "\n",
    "\n",
    "    def one_hot(self, y, gpu):\n",
    "\n",
    "        try:\n",
    "            y = th.from_numpy(y)\n",
    "        except TypeError:\n",
    "            None\n",
    "\n",
    "        y_1d = y\n",
    "        if gpu:\n",
    "            y_hot = th.zeros((y.size(0), th.max(y).int()+1)).cuda()\n",
    "        else:\n",
    "            y_hot = th.zeros((y.size(0), th.max(y).int()+1))\n",
    "\n",
    "        for i in range(y.size(0)):\n",
    "            y_hot[i, y_1d[i].int()] = 1\n",
    "\n",
    "        return y_hot\n",
    "\n",
    "   \n",
    "    def best_tetr_acc(self,prunes):\n",
    "\n",
    "      print(\"prunes vaues id \",prunes)\n",
    "      tr_acc=self.train_accuracy[prunes:]\n",
    "      te_acc=self.test_accuracy[prunes:]\n",
    "      best_te_acc=max(te_acc)\n",
    "      indices = [i for i, x in enumerate(te_acc) if x == best_te_acc]\n",
    "      temp_tr_acc=[]\n",
    "      for i in indices:\n",
    "         temp_tr_acc.append(tr_acc[i])\n",
    "      best_tr_acc=max(temp_tr_acc)\n",
    "      \n",
    "      del self.test_accuracy[prunes:]\n",
    "      del self.train_accuracy[prunes:]\n",
    "      self.test_accuracy.append(best_te_acc)\n",
    "      self.train_accuracy.append(best_tr_acc)\n",
    "      return best_te_acc,best_tr_acc\n",
    "\n",
    "    def best_tetr_acc(self):\n",
    "\n",
    "      tr_acc=self.train_accuracy[:]\n",
    "      te_acc=self.test_accuracy[:]\n",
    "      best_te_acc=max(te_acc)\n",
    "      indices = [i for i, x in enumerate(te_acc) if x == best_te_acc]\n",
    "      temp_tr_acc=[]\n",
    "      for i in indices:\n",
    "         temp_tr_acc.append(tr_acc[i])\n",
    "      best_tr_acc=max(temp_tr_acc)\n",
    "      \n",
    "      del self.test_accuracy[prunes:]\n",
    "      del self.train_accuracy[prunes:]\n",
    "      self.test_accuracy.append(best_te_acc)\n",
    "      self.train_accuracy.append(best_tr_acc)\n",
    "      return best_te_acc,best_tr_acc\n",
    "\n",
    "    \n",
    "    def create_folders(self,total_convs):\n",
    "\n",
    "      main_dir=strftime(\"/Results/%b%d_%H:%M:%S%p\", localtime() )+\"_resnet_56/\"\n",
    "      import os\n",
    "      current_dir =  os.path.abspath(os.path.dirname(__file__))\n",
    "      par_dir = os.path.abspath(current_dir + \"/../\")\n",
    "      parent_dir=par_dir+main_dir\n",
    "      path2=os.path.join(parent_dir, \"layer_file_info\")\n",
    "      os.makedirs(path2)\n",
    "      return parent_dir\n",
    "\n",
    "    def get_writerow(self,k):\n",
    "\n",
    "      s='wr.writerow(['\n",
    "\n",
    "      for i in range(k):\n",
    "\n",
    "          s=s+'d['+str(i)+']'\n",
    "\n",
    "          if(i<k-1):\n",
    "             s=s+','\n",
    "          else:\n",
    "             s=s+'])'\n",
    "\n",
    "      return s\n",
    "\n",
    "    def get_logger(self,file_path):\n",
    "\n",
    "        logger = logging.getLogger('gal')\n",
    "        log_format = '%(asctime)s | %(message)s'\n",
    "        formatter = logging.Formatter(log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "        file_handler = logging.FileHandler(file_path)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(formatter)\n",
    "\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(stream_handler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b887e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PruningMethod:\n",
    "    def prune_filters(self, indices):\n",
    "        conv_layer = 0\n",
    "        for layer_name, layer_module in self.named_modules():\n",
    "            if isinstance(layer_module, th.nn.Conv2d) and 'conv' in layer_name:\n",
    "                if layer_name == 'conv1':\n",
    "                    in_channels = list(range(layer_module.weight.shape[1]))\n",
    "                    out_channels = indices[conv_layer]\n",
    "                    new_weight = th.nn.Parameter(layer_module.weight.data[out_channels].clone().to('cuda'))\n",
    "                    layer_module.weight = new_weight\n",
    "                elif layer_name.startswith('conv2'):\n",
    "                    in_channels = indices[conv_layer - 1]\n",
    "                    out_channels = list(range(layer_module.weight.shape[0]))\n",
    "                    new_weight = th.nn.Parameter(layer_module.weight.data[:, in_channels].clone().to('cuda'))\n",
    "                    layer_module.weight = new_weight\n",
    "                    conv_layer += 1\n",
    "                \n",
    "                layer_module.in_channels = len(in_channels)\n",
    "                layer_module.out_channels = len(out_channels)\n",
    "            \n",
    "            elif isinstance(layer_module, th.nn.BatchNorm2d) and 'bn' in layer_name:\n",
    "                out_channels = indices[conv_layer]\n",
    "                new_weight = th.nn.Parameter(layer_module.weight.data[out_channels].clone().to('cuda'))\n",
    "                new_bias = th.nn.Parameter(layer_module.bias.data[out_channels].clone().to('cuda'))\n",
    "                layer_module.weight = new_weight\n",
    "                layer_module.bias = new_bias\n",
    "                layer_module.running_mean = layer_module.running_mean.clone().to('cuda')\n",
    "                layer_module.running_var = layer_module.running_var.clone().to('cuda')\n",
    "                layer_module.num_features = len(out_channels)\n",
    "                conv_layer += 1\n",
    "\n",
    "\n",
    "    def get_indices_topk(self, layer_bounds, layer_num, prune_limit, prune_value):\n",
    "        i = layer_num\n",
    "        indices = prune_value[i]\n",
    "\n",
    "        p = len(layer_bounds)\n",
    "        if (p - indices) < prune_limit:\n",
    "            prune_value[i] = p - prune_limit\n",
    "            indices = prune_value[i]\n",
    "\n",
    "        k = sorted(range(len(layer_bounds)), key=lambda j: layer_bounds[j])[:indices]\n",
    "        return k\n",
    "\n",
    "    def get_indices_bottomk(self, layer_bounds, layer_num, prune_limit):\n",
    "        k = sorted(range(len(layer_bounds)), key=lambda j: layer_bounds[j])[-prune_limit:]\n",
    "        return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0d25b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class ResBasicBlock(nn.Module,Network,PruningMethod):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super(ResBasicBlock, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.planes = planes\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.stride = stride\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inplanes != planes:\n",
    "            self.shortcut = LambdaLayer(\n",
    "                lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes // 4, planes-inplanes-(planes//4)), \"constant\", 0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module,Network,PruningMethod):\n",
    "    def __init__(self, block, num_layers, covcfg,num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert (num_layers - 2) % 6 == 0, 'depth should be 6n+2'\n",
    "        n = (num_layers - 2) // 6\n",
    "        self.covcfg = covcfg\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(1,block, 16, blocks=n, stride=1)\n",
    "        self.layer2 = self._make_layer(2,block, 32, blocks=n, stride=2)\n",
    "        self.layer3 = self._make_layer(3,block, 64, blocks=n, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        if num_layers == 110:\n",
    "            self.linear = nn.Linear(64 * block.expansion, num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        self.initialize()\n",
    "        self.layer_name_num={}\n",
    "        self.pruned_filters={}\n",
    "        self.remaining_filters={}\n",
    "\n",
    "        self.remaining_filters_each_epoch=[]\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self,a, block, planes, blocks, stride):\n",
    "        layers = [] \n",
    "\n",
    "        layers.append(block(self.inplanes, planes, stride))\n",
    "\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.num_layers == 110:\n",
    "            x = self.linear(x)\n",
    "        else:\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet_56():\n",
    "    cov_cfg = [(3 * i + 2) for i in range(9 * 3 * 2 + 1)]\n",
    "    return ResNet(ResBasicBlock, 56, cov_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c9e1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruningResNet(ResNet, PruningMethod):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ad37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = resnet_56()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee97df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and scheduler\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = th.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30], gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c3c4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model if available\n",
    "checkpoint = th.load('resnet56_base.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "epoch_train_acc = checkpoint['train_acc']\n",
    "epoch_test_acc = checkpoint['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98acbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get convolutional layers\n",
    "conv_layers = [module for module in model.modules() if isinstance(module, nn.Conv2d)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71752288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate cosine similarity between filters\n",
    "def calculate_cosine_similarity(layer_weights):\n",
    "    num_filters = layer_weights.shape[0]\n",
    "    flat_filters = layer_weights.reshape(num_filters, -1).cpu().numpy()\n",
    "    similarity_matrix = cosine_similarity(flat_filters)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaabf065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning loop\n",
    "continue_pruning = True\n",
    "prunes = 0\n",
    "best_train_acc = epoch_train_acc\n",
    "best_test_acc = epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfcba989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'inputs' and 'targets' are already on 'device' (cuda or cpu)\n",
    "for inputs, targets in trainloader:\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # Move entire model to GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "while continue_pruning:\n",
    "    # Calculate cosine similarity for each layer\n",
    "    layer_similarities = []\n",
    "    for layer in conv_layers:\n",
    "        with th.no_grad():\n",
    "            similarity_matrix = calculate_cosine_similarity(layer.weight)\n",
    "            layer_similarities.append(similarity_matrix)\n",
    "\n",
    "    # Select filters to prune based on cosine similarity\n",
    "    inc_indices = []\n",
    "    unimp_indices = []\n",
    "    dec_indices = []\n",
    "    remaining_indices = []\n",
    "\n",
    "    for i, sim_matrix in enumerate(layer_similarities):\n",
    "        num_filters = sim_matrix.shape[0]\n",
    "        sim_flat = sim_matrix.flatten()\n",
    "        sorted_indices = np.argsort(sim_flat)[::-1]\n",
    "        selected_indices = []\n",
    "        for idx in sorted_indices:\n",
    "            if i < len(prune_limits) and len(selected_indices) >= prune_limits[i]:\n",
    "                break\n",
    "            row, col = divmod(idx, num_filters)\n",
    "            if row != col and row not in selected_indices and col not in selected_indices:\n",
    "                selected_indices.extend([row, col])\n",
    "            i += 1  # Increment i inside the loop if applicable\n",
    "\n",
    "        inc_indices.append(selected_indices)\n",
    "\n",
    "        # Ensure i is within the valid range of prune_limits\n",
    "        if i < len(prune_limits):\n",
    "            unimp_indices_layer = model.get_indices_topk(sim_matrix.sum(axis=0).tolist(), min(i, len(prune_limits) - 1), prune_limits[min(i, len(prune_limits) - 1)], prune_percentage)\n",
    "        else:\n",
    "            unimp_indices_layer = []\n",
    "        unimp_indices.append(unimp_indices_layer)\n",
    "        dec_indices.append(list(set(selected_indices + unimp_indices_layer)))\n",
    "        remaining_indices.append([j for j in range(num_filters) if j not in unimp_indices_layer])\n",
    "\n",
    "    # Custom regularization\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=optim_lr, momentum=0.9)\n",
    "    for epoch in range(custom_epochs):\n",
    "        train_acc = []\n",
    "        for inputs, targets in trainloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate regularization terms\n",
    "            reg = th.zeros(1).to(device)\n",
    "            for i, layer in enumerate(conv_layers):\n",
    "                dec_weight = sum(layer.weight[idx].norm(1) for idx in dec_indices[i])\n",
    "                inc_weight = sum(layer.weight[idx].norm(1) for idx in inc_indices[i])\n",
    "                reg += lamda * (dec_weight - inc_weight)\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets) + reg\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with th.no_grad():\n",
    "                y_hat = th.argmax(output, 1)\n",
    "                train_acc.append((y_hat == targets).sum().item())\n",
    "\n",
    "        epoch_train_acc = sum(train_acc) * 100 / len(trainloader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{custom_epochs}], Train Accuracy: {epoch_train_acc:.2f}%')\n",
    "\n",
    "        test_acc = []\n",
    "        with th.no_grad():\n",
    "            for inputs, targets in testloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                output = model(inputs)\n",
    "                y_hat = th.argmax(output, 1)\n",
    "                test_acc.append((y_hat == targets).sum().item())\n",
    "        epoch_test_acc = sum(test_acc) * 100 / len(testloader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{custom_epochs}], Test Accuracy: {epoch_test_acc:.2f}%')\n",
    "\n",
    "        if epoch_test_acc > best_test_acc:\n",
    "            best_train_acc = epoch_train_acc\n",
    "            best_test_acc = epoch_test_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "            best_opt_wts = optimizer.state_dict()\n",
    "            best_sch_wts = scheduler.state_dict()\n",
    "\n",
    "    # Prune filters\n",
    "    model.prune_filters(remaining_indices)\n",
    "\n",
    "    # Print remaining filters in each convolutional layer\n",
    "    for i, layer in enumerate(conv_layers):\n",
    "        print(f'Layer {i+1} - Remaining Filters: {layer.out_channels}')\n",
    "\n",
    "    # Check if desired filter counts are reached\n",
    "    continue_pruning = any(layer.out_channels > prune_limits[i] for i, layer in enumerate(conv_layers))\n",
    "\n",
    "    # Fine-tuning\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=optim_lr, momentum=0.9)\n",
    "    scheduler = th.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20], gamma=0.1)\n",
    "    for epoch in range(new_epochs):\n",
    "        train_acc = []\n",
    "        for inputs, targets in trainloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with th.no_grad():\n",
    "                y_hat = th.argmax(output, 1)\n",
    "                train_acc.append((y_hat == targets).sum().item())\n",
    "\n",
    "        epoch_train_acc = sum(train_acc) * 100 / len(trainloader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{new_epochs}], Train Accuracy: {epoch_train_acc:.2f}%')\n",
    "\n",
    "        test_acc = []\n",
    "        with th.no_grad():\n",
    "            for inputs, targets in testloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                output = model(inputs)\n",
    "                y_hat = th.argmax(output, 1)\n",
    "                test_acc.append((y_hat == targets).sum().item())\n",
    "        epoch_test_acc = sum(test_acc) * 100 / len(testloader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{new_epochs}], Test Accuracy: {epoch_test_acc:.2f}%')\n",
    "\n",
    "        if epoch_test_acc > best_test_acc:\n",
    "            best_train_acc = epoch_train_acc\n",
    "            best_test_acc = epoch_test_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "            best_opt_wts = optimizer.state_dict()\n",
    "            best_sch_wts = scheduler.state_dict()\n",
    "\n",
    "    prunes += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pruned model\n",
    "th.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "    'train_acc': best_train_acc,\n",
    "    'test_acc': best_test_acc\n",
    "}, 'pruned_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pruning completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d7e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate FLOPs and parameters\n",
    "dummy_input = th.randn(1, 1, 28, 28).to(device)\n",
    "flops, params = profile(model, inputs=(dummy_input,))\n",
    "print(f\"Total FLOPs: {flops}, Total Params: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4cefb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
